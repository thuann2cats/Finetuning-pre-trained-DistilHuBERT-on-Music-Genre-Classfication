{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, I fine-tuned a pre-trained transformer model, `DistilHuBERT`, for music genre classification using the `GTZAN` dataset, a collection of 1,000 30-second music clips across 10 genres. (I followed a tutorial in the Hugging Face Audio course, but the code here was my own. I learned the general data preprocessing and training approach and wrote my own scripts.)\n",
    "\n",
    "With the feature extractor that came with the model, I resampled the audio to the appropriate frequency, and normalized audio data to zero mean and unit variance. The model was then fine-tuned to predict music genres from raw audio waveforms. \n",
    "\n",
    "For demonstration purpose, the model was trained for 10 epochs and achieved about 80% accuracy on a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "normal_repr = torch.Tensor.__repr__ \n",
    "torch.Tensor.__repr__ = lambda self: f\"{self.shape}_{normal_repr(self)}\"  \n",
    "\n",
    "def info(obj, name=None):\n",
    "    \"\"\"Inspects an object and prints its details.\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"*\" * 20)\n",
    "    print(\"Variable name: \", \"<unknown>\" if not name else name)\n",
    "    #   print(\"Object name:\", obj.__name__ if hasattr(obj, '__name__') else str(obj))\n",
    "    print(\"Object type:\", type(obj))\n",
    "\n",
    "    if hasattr(obj, 'keys'):\n",
    "        print(\"Number of keys:\", len(obj.keys()))\n",
    "        print(\"Keys:\", list(obj.keys()))\n",
    "    if hasattr(obj, '__len__'):\n",
    "        print(\"Length:\", len(obj))\n",
    "    if isinstance(obj, (np.ndarray, torch.Tensor)):\n",
    "        print(\"Shape:\", obj.shape)\n",
    "    else:\n",
    "        # print(\"Other object type:\", obj)\n",
    "        pass\n",
    "    print(\"*\" * 20 + \"\\n\")\n",
    "\n",
    "# Example usage\n",
    "# my_list = [1, 2, 3, 4]\n",
    "# my_dict = {'a': 10, 'b': [1, 2, 3], 'c': np.array([1, 2, 3])}\n",
    "# my_tensor = torch.randn(3, 4)\n",
    "\n",
    "# info(my_list)\n",
    "# info(my_dict)\n",
    "# info(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_checkpoint = \"ntu-spml/distilhubert\"\n",
    "dataset_id = \"marsyas/gtzan\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading gtzan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gtzan_original = load_dataset(dataset_id)\n",
    "dataset_gtzan_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_fn = dataset_gtzan_original[\"train\"].features[\"genre\"].int2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/nguyenthuan49/.cache/huggingface/datasets/downloads/extracted/c96fd9bab3c30c67977bca7a5c5f8bba015190c35962bb0ab4780546891dd836/genres/blues/blues.00000.wav',\n",
       " 'audio': {'path': '/home/nguyenthuan49/.cache/huggingface/datasets/downloads/extracted/c96fd9bab3c30c67977bca7a5c5f8bba015190c35962bb0ab4780546891dd836/genres/blues/blues.00000.wav',\n",
       "  'array': array([ 0.00732422,  0.01660156,  0.00762939, ..., -0.05560303,\n",
       "         -0.06106567, -0.06417847]),\n",
       "  'sampling_rate': 22050},\n",
       " 'genre': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gtzan_original[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 799\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "dataset_gtzan_original_shuffled: Dataset = dataset_gtzan_original[\"train\"].shuffle()   # .select(range(50))\n",
    "\n",
    "dataset_gtzan_original_split = dataset_gtzan_original_shuffled.train_test_split(test_size=0.2, seed=42, )\n",
    "\n",
    "dataset_gtzan_mini = dataset_gtzan_original_split\n",
    "\n",
    "dataset_gtzan_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def generate_audio(idx):\n",
    "    example = dataset_gtzan_mini[\"train\"][idx]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label_fn(example[\"genre\"])\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for i in range(0, 20):\n",
    "            audio, label = generate_audio(i)\n",
    "            output = gr.Audio(audio, label=label)\n",
    "            \n",
    "demo.launch(debug=True, share=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0,\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_checkpoint,\n",
    "                                                         do_normalize=True,\n",
    "                                                         return_attention_mask=True)\n",
    "feature_extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gtzan_train = dataset_gtzan_mini[\"train\"]\n",
    "dataset_gtzan_test = dataset_gtzan_mini[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = (Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "# info(a)\n",
    "\n",
    "# a.__call__(dataset_gtzan_mini[\"train\"][0][\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "dataset_gtzan_mini = dataset_gtzan_mini.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_values': [array([-1.3018837 , -0.39399064,  0.25152016,  1.4443543 ], dtype=float32), array([ 0.91363186, -0.85418344, -1.1328534 ,  1.073405  ], dtype=float32)], 'attention_mask': [array([1, 1, 1, 1], dtype=int32), array([1, 1, 1, 1], dtype=int32)]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor([dataset_gtzan_train[\"audio\"][i][\"array\"] for i in range(2)], sampling_rate=feature_extractor.sampling_rate, truncation=True, max_length=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = 20\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    audio_arrays = [audio_sample[\"array\"] for audio_sample in batch[\"audio\"]]\n",
    "    encoded_batch = feature_extractor(audio_arrays,\n",
    "                      truncation=True,\n",
    "                      sampling_rate=feature_extractor.sampling_rate,\n",
    "                      max_length=max_duration * feature_extractor.sampling_rate)\n",
    "    return encoded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10efeb5535c7460e9e428604f70b842b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['genre', 'input_values', 'attention_mask'],\n",
       "    num_rows: 799\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gtzan_train_encoded = dataset_gtzan_train.map(preprocess_function,\n",
    "                                                      batched=True,\n",
    "                                                      batch_size=50,\n",
    "                                                      remove_columns=[\"audio\", \"file\"])\n",
    "dataset_gtzan_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gtzan_train_encoded = dataset_gtzan_train_encoded.rename_column(original_column_name=\"genre\", new_column_name=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_values', 'attention_mask'],\n",
       "    num_rows: 799\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_gtzan_train_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0cea0d06844eeebc50cdb86da67945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_values', 'attention_mask'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_gtzan_test_encoded = dataset_gtzan_test.map(preprocess_function,\n",
    "                                                    batched=True,\n",
    "                                                    batch_size=50,\n",
    "                                                    remove_columns=[\"audio\", \"file\"])\n",
    "\n",
    "dataset_gtzan_test_encoded = dataset_gtzan_test_encoded.rename_column(\"genre\", \"label\")\n",
    "dataset_gtzan_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating id2label and label2id, needed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 'blues',\n",
       "  '1': 'classical',\n",
       "  '2': 'country',\n",
       "  '3': 'disco',\n",
       "  '4': 'hiphop',\n",
       "  '5': 'jazz',\n",
       "  '6': 'metal',\n",
       "  '7': 'pop',\n",
       "  '8': 'reggae',\n",
       "  '9': 'rock'},\n",
       " {'blues': '0',\n",
       "  'classical': '1',\n",
       "  'country': '2',\n",
       "  'disco': '3',\n",
       "  'hiphop': '4',\n",
       "  'jazz': '5',\n",
       "  'metal': '6',\n",
       "  'pop': '7',\n",
       "  'reggae': '8',\n",
       "  'rock': '9'})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(dataset_gtzan_original[\"train\"].features[\"genre\"].names)\n",
    "\n",
    "id2label = {\n",
    "    str(i): id2label_fn(i) for i in range(num_labels)\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    v: k for k, v in id2label.items()\n",
    "}\n",
    "\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['classifier.bias', 'classifier.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_checkpoint,\n",
    "                                                        num_labels=num_labels,\n",
    "                                                        id2label=id2label,\n",
    "                                                        label2id=label2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/HF_NLP/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "gradient_accumulation_batches = 1\n",
    "num_train_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"hubert-finetuned-gtzan\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    # eval_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_batches,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "import numpy as np \n",
    "\n",
    "def compute_metrics(eval_outputs):\n",
    "    predictions = np.argmax(eval_outputs.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_outputs.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/HF_NLP/lib/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=dataset_gtzan_train_encoded, \n",
    "    eval_dataset=dataset_gtzan_test_encoded, \n",
    "    tokenizer=feature_extractor, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 1:00:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.219600</td>\n",
       "      <td>1.958643</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.685100</td>\n",
       "      <td>1.379236</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.252600</td>\n",
       "      <td>1.063894</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.948100</td>\n",
       "      <td>0.888793</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.812161</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.516700</td>\n",
       "      <td>0.702959</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.676287</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.675458</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.618748</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.642744</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.8364749298095703, metrics={'train_runtime': 3656.0458, 'train_samples_per_second': 2.185, 'train_steps_per_second': 0.274, 'total_flos': 3.634450598016e+17, 'train_loss': 0.8364749298095703, 'epoch': 10.0})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
